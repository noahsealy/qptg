{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qtpg.team import Team\n",
    "from qtpg.agent import Agent\n",
    "from qtpg.trainer import Trainer\n",
    "from qtpg.figure13 import Figure13\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp params\n",
    "numLearners = 4\n",
    "numAgents = 20\n",
    "numRuns = 1\n",
    "numGens = 100\n",
    "numEpisodes = 50\n",
    "gap = 0.3\n",
    "# rl params\n",
    "alpha, discount, epsilon = 0.9, 0.9, 0.1\n",
    "# env params\n",
    "memorySize = 20\n",
    "legalMove = 0.1\n",
    "illegalMove = -0.01\n",
    "outOfBounds = -0.01\n",
    "memoryRepeat = 0#-0.01\n",
    "goalReached = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(gap, numLearners, numAgents, alpha, discount, epsilon)\n",
    "# init env\n",
    "env = Figure13(5, 5, (2, 4), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "env.memoryRepeat = 0.0\n",
    "# search space to make rules\n",
    "trainer.generateRules(env)\n",
    "print(trainer.rules)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for run in range(numRuns):\n",
    "    # init agent population\n",
    "    trainer = Trainer(gap, numLearners, numAgents, alpha, discount, epsilon)\n",
    "    \n",
    "    # init env\n",
    "    env = Figure13(5, 5, (2, 4), (4, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "    env.memoryRepeat = 0.0\n",
    "    # search space to make rules\n",
    "    trainer.generateRules(env)\n",
    "    \n",
    "    trainer.createInitAgents()\n",
    "    env.memoryRepeat = -0.01\n",
    "    champs = []\n",
    "    for gen in range(numGens):\n",
    "        print(gen)\n",
    "        env.reset()\n",
    "        for agent in trainer.agents:\n",
    "            print(agent.win)\n",
    "            env.reset()\n",
    "            episode_fitness = [] # new\n",
    "            for ep in range(numEpisodes):\n",
    "                if agent.win is not True:\n",
    "                    env.reset()\n",
    "                    fitness, win, states, seq = agent.evaluate_fitness(env)\n",
    "                    episode_fitness.append(fitness)\n",
    "#                     print(agent.team.q_table)\n",
    "                    if win == True:\n",
    "                        print('win stuff start')\n",
    "                        print(states)\n",
    "                        agent.win = True\n",
    "                        for q_value in agent.team.q_table:\n",
    "                            print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "                        for i in range(5):\n",
    "                            env.reset()\n",
    "                            agent.team.victory_lap(env, seq)\n",
    "                        print('\\n\\n\\n')\n",
    "                        for q_value in agent.team.q_table:\n",
    "                            print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "                        env.reset()\n",
    "#                         agent.team.epsilon = 0.0\n",
    "                        score, win, states = agent.replay(env)\n",
    "                        print(agent.team.q_table)\n",
    "                        print('win stuff end!')\n",
    "                        print('REPLAY')\n",
    "                        print(score)\n",
    "                        print(win)\n",
    "                        if not win:\n",
    "                            agent.win = False\n",
    "                        else:\n",
    "                            champs.append(agent)\n",
    "                            print('win replay!')\n",
    "                        \n",
    "                        print(states)\n",
    "                        print('!!!!')\n",
    "            # average fitness over episodes...\n",
    "            average_fitness = 0\n",
    "            average_fitness = np.mean(episode_fitness)\n",
    "            print(average_fitness)\n",
    "            agent.fitness = average_fitness\n",
    "        trainer.evolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(champs))\n",
    "champ_index = 0\n",
    "champs[champ_index].team.epsilon = 0.0\n",
    "\n",
    "for q_value in champs[champ_index].team.q_table:\n",
    "    print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "\n",
    "wins = 0\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    score, win, states = champs[champ_index].replay(env)\n",
    "    print(win)\n",
    "    if win:\n",
    "        print(states)\n",
    "        wins += 1\n",
    "    print(score)\n",
    "print(wins)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
