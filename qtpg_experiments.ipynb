{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from qtpg.team import Team\n",
    "from qtpg.agent import Agent\n",
    "from qtpg.trainer import Trainer\n",
    "from qtpg.figure13 import Figure13\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gp params\n",
    "numLearners = 4\n",
    "numAgents = 1\n",
    "numRuns = 1\n",
    "numGens = 100\n",
    "numEpisodes = 50\n",
    "gap = 0.3\n",
    "# rl params\n",
    "alpha, discount, epsilon = 0.9, 0.9, 0.1\n",
    "# env params\n",
    "memorySize = 20\n",
    "legalMove = 0.1\n",
    "illegalMove = -0.01\n",
    "outOfBounds = -0.01\n",
    "memoryRepeat = 0#-0.01\n",
    "goalReached = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search tests\n",
    "max_rules = 5\n",
    "\n",
    "trainer = Trainer(gap, numLearners, numAgents, max_rules, alpha, discount, epsilon)\n",
    "trainer.createInitAgents()\n",
    "env = Figure13(5, 5, (2, 4), (0, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "env.memoryRepeat = 0.0\n",
    "for agent in trainer.agents:\n",
    "    for gen in range(numGens):\n",
    "        selected_rule = agent.team.select_rule()\n",
    "        offspring = agent.team.search(selected_rule, env)\n",
    "        agent.team.evaluate_rule(offspring)\n",
    "    env.reset()\n",
    "    print(agent.team.rule_pool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d25cef5c-4fd2-470e-aace-7835b53fb59c\n",
      "[1, 0, 2, 0]\n",
      "(0, 1)\n",
      "7afba89e-e0a1-4eb5-b62b-f4a37af85472\n",
      "[0, 1, 3, -1]\n",
      "(3, 1)\n",
      "d83bffac-5867-47d7-a258-0ed4fb9ddfaf\n",
      "[1, 2, 1, 3]\n",
      "(0, 1)\n",
      "65b83b81-9efc-4358-8d1f-ac9a8cd63040\n",
      "[0, 4, 2, 3]\n",
      "(2, 1)\n",
      "2fc83574-6a06-4e6a-a6e9-f0c632a80c99\n",
      "[1, 4, 2, 4]\n",
      "(1, 1)\n",
      "5\n",
      "5\n",
      "start!\n",
      "[1, 0, 2, 0]\n",
      "[0, 1, 3, -1]\n",
      "[1, 2, 1, 3]\n",
      "[0, 4, 2, 3]\n",
      "[1, 4, 2, 4]\n",
      "end!\n",
      "0\n",
      "False\n",
      "0.10659999999999953\n",
      "False\n",
      "0.15499999999999958\n",
      "False\n",
      "0.15059999999999957\n",
      "False\n",
      "0.13079999999999958\n",
      "False\n",
      "0.1901999999999995\n",
      "False\n",
      "0.13739999999999963\n",
      "False\n",
      "0.14619999999999952\n",
      "False\n",
      "0.09339999999999948\n",
      "False\n",
      "0.08459999999999958\n",
      "False\n",
      "0.07359999999999958\n",
      "False\n",
      "0.1373999999999995\n",
      "False\n",
      "0.08899999999999957\n",
      "False\n",
      "0.08019999999999956\n",
      "False\n",
      "0.1307999999999996\n",
      "False\n",
      "0.09999999999999963\n",
      "False\n",
      "0.06259999999999956\n",
      "False\n",
      "0.16599999999999956\n",
      "False\n",
      "0.11099999999999953\n",
      "False\n",
      "0.18579999999999952\n",
      "False\n",
      "0.2737999999999995\n",
      "1\n",
      "False\n",
      "0.17259999999999953\n",
      "False\n",
      "0.13519999999999952\n",
      "False\n",
      "0.016399999999999536\n",
      "False\n",
      "0.22319999999999954\n",
      "False\n",
      "0.06259999999999952\n",
      "False\n",
      "0.23859999999999956\n",
      "False\n",
      "0.060399999999999565\n",
      "False\n",
      "0.19459999999999955\n",
      "False\n",
      "0.07579999999999956\n",
      "False\n",
      "0.07139999999999974\n",
      "False\n",
      "0.16379999999999956\n",
      "False\n",
      "0.09779999999999957\n",
      "False\n",
      "0.10439999999999959\n",
      "False\n",
      "0.08679999999999959\n",
      "False\n",
      "0.2077999999999996\n",
      "False\n",
      "0.1967999999999995\n",
      "False\n",
      "0.1879999999999995\n",
      "False\n",
      "0.1593999999999996\n",
      "False\n",
      "0.07139999999999957\n",
      "False\n",
      "0.09559999999999957\n",
      "2\n",
      "False\n",
      "0.20339999999999958\n",
      "False\n",
      "0.07139999999999957\n",
      "False\n",
      "0.17259999999999953\n",
      "False\n",
      "0.08239999999999956\n",
      "False\n",
      "0.17259999999999961\n",
      "False\n",
      "0.08459999999999951\n",
      "False\n",
      "0.1263999999999995\n",
      "False\n",
      "0.1879999999999995\n",
      "False\n",
      "0.19019999999999956\n",
      "False\n",
      "0.06699999999999962\n",
      "False\n",
      "0.22979999999999953\n",
      "False\n",
      "0.06259999999999964\n",
      "False\n",
      "0.020799999999999597\n",
      "False\n",
      "0.2407999999999995\n",
      "False\n",
      "0.10439999999999959\n",
      "False\n",
      "0.10659999999999954\n",
      "False\n",
      "0.20119999999999952\n",
      "False\n",
      "0.07579999999999953\n",
      "False\n",
      "0.20559999999999953\n",
      "False\n",
      "0.08459999999999958\n",
      "3\n",
      "False\n",
      "0.15059999999999962\n",
      "False\n",
      "0.12639999999999948\n",
      "False\n",
      "0.10659999999999956\n",
      "False\n",
      "0.13739999999999955\n",
      "False\n",
      "0.16379999999999956\n",
      "False\n",
      "0.1395999999999996\n",
      "False\n",
      "0.04939999999999963\n",
      "False\n",
      "0.16159999999999958\n",
      "False\n",
      "0.0713999999999996\n",
      "False\n",
      "0.03399999999999963\n",
      "False\n",
      "0.18579999999999952\n",
      "False\n",
      "0.14619999999999955\n",
      "False\n",
      "0.11539999999999952\n",
      "False\n",
      "0.060399999999999565\n",
      "False\n",
      "0.13079999999999956\n",
      "False\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-53f4e805db57>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     22\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwin\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m                     \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                     \u001b[0mfitness\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwin\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate_fitness\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                     \u001b[0mepisode_fitness\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfitness\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;31m#                     print(agent.team.q_table)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/thesis/PyQTPG/qtpg/agent.py\u001b[0m in \u001b[0;36mevaluate_fitness\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     26\u001b[0m                 \u001b[0;31m#     learner.program.display()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mtotal_reward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m             \u001b[0ml_next\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ma_next\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mteam\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ml_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0ml_next\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Desktop/thesis/PyQTPG/qtpg/team.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtop_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mentry\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_table\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'learner'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtop_bid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m                 \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'action'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mentry\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'q'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mtop_q\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.8/uuid.py\u001b[0m in \u001b[0;36m__str__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    271\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__str__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m         \u001b[0mhex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'%032x'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 273\u001b[0;31m         return '%s-%s-%s-%s-%s' % (\n\u001b[0m\u001b[1;32m    274\u001b[0m             hex[:8], hex[8:12], hex[12:16], hex[16:20], hex[20:])\n\u001b[1;32m    275\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for run in range(numRuns):\n",
    "    # init agent population\n",
    "    trainer = Trainer(gap, numLearners, numAgents, alpha, discount, epsilon)\n",
    "    \n",
    "    # init env\n",
    "    env = Figure13(5, 5, (2, 4), (0, 0), memorySize, legalMove, illegalMove, outOfBounds, memoryRepeat, goalReached)\n",
    "    env.memoryRepeat = 0.0\n",
    "    # search space to make rules\n",
    "    trainer.generateRules(env)\n",
    "    env.memoryRepeat = -0.01    \n",
    "    trainer.createInitAgents()\n",
    "\n",
    "    champs = []\n",
    "    for gen in range(numGens):\n",
    "        print(gen)\n",
    "        env.reset()\n",
    "        for agent in trainer.agents:\n",
    "            print(agent.win)\n",
    "            env.reset()\n",
    "            episode_fitness = [] # new\n",
    "            for ep in range(numEpisodes):\n",
    "                if agent.win is not True:\n",
    "                    env.reset()\n",
    "                    fitness, win, states, seq = agent.evaluate_fitness(env)\n",
    "                    episode_fitness.append(fitness)\n",
    "#                     print(agent.team.q_table)\n",
    "                    if win == True:\n",
    "                        print('win stuff start')\n",
    "                        print(states)\n",
    "                        agent.win = True\n",
    "                        for q_value in agent.team.q_table:\n",
    "                            print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "                        for i in range(5):\n",
    "                            env.reset()\n",
    "                            agent.team.victory_lap(env, seq)\n",
    "                        print('\\n\\n\\n')\n",
    "                        for q_value in agent.team.q_table:\n",
    "                            print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "                        env.reset()\n",
    "#                         agent.team.epsilon = 0.0\n",
    "                        score, win, states = agent.replay(env)\n",
    "                        print(agent.team.q_table)\n",
    "                        print('win stuff end!')\n",
    "                        print('REPLAY')\n",
    "                        print(score)\n",
    "                        print(win)\n",
    "                        if not win:\n",
    "                            agent.win = False\n",
    "                        else:\n",
    "                            champs.append(agent)\n",
    "                            print('win replay!')\n",
    "                        \n",
    "                        print(states)\n",
    "                        print('!!!!')\n",
    "            # average fitness over episodes...\n",
    "            average_fitness = 0\n",
    "            average_fitness = np.mean(episode_fitness)\n",
    "            print(average_fitness)\n",
    "            agent.fitness = average_fitness\n",
    "        trainer.evolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(len(champs))\n",
    "champ_index = 0\n",
    "champs[champ_index].team.epsilon = 0.0\n",
    "\n",
    "for q_value in champs[champ_index].team.q_table:\n",
    "    print(str(q_value['learner']) + ' ' + str(q_value['action']) + ' ' + str(q_value['q']))\n",
    "\n",
    "wins = 0\n",
    "for i in range(100):\n",
    "    env.reset()\n",
    "    score, win, states = champs[champ_index].replay(env)\n",
    "    print(win)\n",
    "    if win:\n",
    "        print(states)\n",
    "        wins += 1\n",
    "    print(score)\n",
    "print(wins)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
